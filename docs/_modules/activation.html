

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>activation &mdash; Explore Neural Networks 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Explore Neural Networks
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">UnderstandingNN</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Explore Neural Networks</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Module code</a> &raquo;</li>
        
      <li>activation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for activation</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Understanding activation functions </span>

<span class="sd">Notes:</span>

<span class="sd">    1. Introducing non linearity to the network. Why?</span>
<span class="sd">    2. According to me we need one parameter to compare all the nodes results after learning and passing the value to upcoming nodes.</span>
<span class="sd">    3. To make sense of the data and a mapping for approximation.</span>
<span class="sd">    4. Understand what is the impact of weights and biases changing value to the network/nodes.</span>
<span class="sd">        If there is only linear fx then it can only fit linear data but if we have not linear data like a sine wave then it will fail to do so. </span>
<span class="sd">    5. If there is no activate function then the whole network will be similar to a one linear node.</span>
<span class="sd">        </span>
<span class="sd">            w.T(w.T *(w.T * x + b) + b) + b ... = output </span>
<span class="sd">            </span>
<span class="sd">=====================================================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<div class="viewcode-block" id="Activation_Stepwise"><a class="viewcode-back" href="../activation.html#activation.Activation_Stepwise">[docs]</a><span class="k">class</span> <span class="nc">Activation_Stepwise</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Stepwise Activation Fx</span>

<span class="sd">    Notes:</span>

<span class="sd">        * non granular </span>
<span class="sd">        * only 0 and 1</span>
<span class="sd">        </span>
<span class="sd">    References:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="Activation_Stepwise.forward"><a class="viewcode-back" href="../activation.html#activation.Activation_Stepwise.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply Stepwise to inputs</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (numpy.ndarray) : input matrix</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="Activation_Sigmoid"><a class="viewcode-back" href="../activation.html#activation.Activation_Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Activation_Sigmoid</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sigmoid Activation Fx</span>

<span class="sd">    Notes:</span>

<span class="sd">        f(x) = 1 / (1 + e^-x)</span>

<span class="sd">        * granular</span>
<span class="sd">        * between 0 and 1</span>
<span class="sd">        * Comparatively complex calcultaion</span>

<span class="sd">    References:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="Activation_Sigmoid.forward"><a class="viewcode-back" href="../activation.html#activation.Activation_Sigmoid.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply Sigmoid to input</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            inputs (numpy.ndarray) : input matrix</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inputs</span><span class="p">))</span></div>

<div class="viewcode-block" id="Activation_Sigmoid.backward"><a class="viewcode-back" href="../activation.html#activation.Activation_Sigmoid.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dvalues</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="Activation_ReLU"><a class="viewcode-back" href="../activation.html#activation.Activation_ReLU">[docs]</a><span class="k">class</span> <span class="nc">Activation_ReLU</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;ReLU Activation Fx</span>

<span class="sd">    Notes:</span>
<span class="sd">            </span>
<span class="sd">        f(x) = 0  if x &lt;= 0</span>
<span class="sd">        f(x) = x  if x &gt; 0</span>

<span class="sd">        * granular</span>
<span class="sd">        * between 0 to x</span>
<span class="sd">        * easy calculation </span>
<span class="sd">        * almost linear but rectified so less than zeros are not allowed.so introducing slight non linearity makes it eligible for an activation function but also inherently easy and fast calculation than sigmoid.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dinputs</span> <span class="o">=</span> <span class="kc">None</span> 

<div class="viewcode-block" id="Activation_ReLU.forward"><a class="viewcode-back" href="../activation.html#activation.Activation_ReLU.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply ReLU to input</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            inputs (numpy.ndarray) : input matrix</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span> <span class="c1"># save inputs </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="c1"># calculate from inputs</span></div>

<div class="viewcode-block" id="Activation_ReLU.backward"><a class="viewcode-back" href="../activation.html#activation.Activation_ReLU.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dvalues</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply backward propogation</span>

<span class="sd">        Args:</span>
<span class="sd">            dvalues (numpy.ndarray) : inputs from previous later in backward prop</span>
<span class="sd">        </span>
<span class="sd">        Notes:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dinputs</span> <span class="o">=</span> <span class="n">dvalues</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dinputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span></div></div>


        


<div class="viewcode-block" id="Activation_Softmax"><a class="viewcode-back" href="../activation.html#activation.Activation_Softmax">[docs]</a><span class="k">class</span> <span class="nc">Activation_Softmax</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Activation_Softmax.forward"><a class="viewcode-back" href="../activation.html#activation.Activation_Softmax.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Forward propogation calculation</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (numpy.ndarray) : input matrix</span>

<span class="sd">        Notes:</span>
<span class="sd">            TODO</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">exp_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">probabilites</span> <span class="o">=</span> <span class="n">exp_values</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">probabilites</span></div>

<div class="viewcode-block" id="Activation_Softmax.backward"><a class="viewcode-back" href="../activation.html#activation.Activation_Softmax.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dvalues</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;backward pass</span>

<span class="sd">        Args:</span>
<span class="sd">            dvalues (numpy.ndarray) : gradient values</span>

<span class="sd">        Notes:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div></div>
        
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Nishant Baheti.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>