{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "<img src='diags/nn_1.dio.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Symbols & Naming Conventions\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    n &= \\text{number of nodes}\\\\\n",
    "    l &= \\text{layer number}\\\\\n",
    "    w,W &= \\text{weights matrix}\\\\\n",
    "    b &= \\text{bias matrix}\\\\\n",
    "    z,Z &= \\text{hypthesis result (result before applying activation function)}\\\\\n",
    "    g(z) &= \\text{activation function}\\\\\n",
    "    a,A &= \\text{activation matrix (result after applying activation function)}\\\\\n",
    "    x,X &= \\text{input to network}\\\\\n",
    "    y &= \\text{output of network}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "values for forward propogation\n",
    "\\begin{align}\n",
    "    \\huge{n^{[l]}} &= \\text{number of nodes in the layer}\\\\\n",
    "    \\huge{z^{[l]}} &= \\text{hypothesis result of the layer}\\\\\n",
    "    \\huge{w^{[l]}} &= \\text{weights results of the layer}\\\\\n",
    "    \\huge{b^{[l]}} &= \\text{bias results of the layer}\\\\\n",
    "    \\huge{a^{[l]}} &= \\text{activation results of the layer}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "derivatives for backward propogation\n",
    "\\begin{align}\n",
    "    \\huge{dw^{[l]}} &= \\frac{\\partial L}{\\partial w} \\rightarrow \\text{loss derivative based on weights}\\\\\n",
    "    \\huge{db^{[l]}} &= \\frac{\\partial L}{\\partial b} \\rightarrow \\text{ loss derivative based on biases}\\\\\n",
    "    \\huge{dz^{[l]}} &= \\frac{\\partial L}{\\partial z} \\rightarrow \\text{ loss derivative based on hypothesis result}\\\\\n",
    "    \\huge{da^{[l]}} &= \\frac{\\partial L}{\\partial a} \\rightarrow \\text{ loss derivative based on activation result}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Shapes \n",
    "\n",
    "\n",
    "\\begin{array}{ | c | c | c | }\n",
    "    \\hline\n",
    "    W^{[l]} & ( n^{[l]} , n^{[l-1]} ) & dW^{[l]} \\\\\n",
    "    \\hline\n",
    "    b^{[l]} & ( n^{[l]} , 1 ) & db^{[l]} \\\\\n",
    "    \\hline\n",
    "    Z^{[l]} & ( n^{[l]} , n^{[l-1]} ) & dZ^{[l]} \\\\\n",
    "    \\hline\n",
    "    A^{[l]} & ( n^{[l]} , n^{[l-1]} ) & dA^{[l]} \\\\\n",
    "    \\hline\n",
    "\\end{array} \n",
    "where\\\n",
    "l = layer number >= 1\\\n",
    "$A^{[0]}$ = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we actually initialize a layer for a New Neural Network?\n",
    "\n",
    "* initialization of weights with small random values\n",
    "\n",
    "    * why? because according to Andrew Ng's explanation if all the weights/params are\n",
    "    initialized by zero or same value then all the hidden units will be symmetric with identical nodes.\n",
    "    \n",
    "    * With identical nodes there will be no learning/ decision making. because all the decisions\n",
    "    shares same value.\n",
    "    \n",
    "    * If all the nodes will have zero values(weights are zero , multiplication with weights will also be zero) and propogation result wont be a conclusive one(dead network).\n",
    "\n",
    "    * shape of weights(theoratically) : (number of neurons, number of inputs)\\\n",
    "    but we have to do transpose operation everytime\n",
    "\n",
    "    * shape of weights(for code) : (number of inputs, number of neurons)\n",
    "\n",
    "    * number of inputs : number of neurons in previous layer or input layer features\n",
    "\n",
    "* initialization of bias can be zero. \n",
    "\n",
    "    * as randomness is already introduced by weights.\n",
    "    But for smaller Neural Network it is advised to not to initialize with zero.\n",
    "\n",
    "    * shape of biases (sentdex) : (1, number of neurons)\n",
    "\n",
    "    * shape of biases (Andrew Ng) :(number of neurons,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Andrew Ng\n",
    "* inputs  : ( m examples, number of input features )\n",
    "* weights : ( number of nodes in previous layer or input features, number of nodes in current layer )\n",
    "* biases  : ( number of nodes in current layer , 1 )\n",
    "* output = weights.T * inputs + biases\n",
    "\\begin{align}\n",
    "    ( n^{[l]} ,m ) = ( n^{[l]} , n^{[l-1]} ) ( n^{[l-1]} , m ) + ( n^{[l]} , 1 )\n",
    "\\end{align}\n",
    "\n",
    "### Sentdex\n",
    "* inputs  : ( m examples, number of input features )\n",
    "* weights : ( number of nodes in previous layer or input features, number of nodes in current layer )\n",
    "* biases  : ( 1, number of nodes in current layer )\n",
    "* output = weights.T * inputs + biases\n",
    "\\begin{align}\n",
    "    ( n^{[l]} , m ) = ( n^{[l]} , n^{[l-1]} ) ( n^{[l-1]} , m ) + ( 1 , n^{[l]} )\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "\n",
    "\\begin{align}\n",
    "    Z^{[1]} &= W^{[1]} A^{[0]} + b^{[1]}\\\\\n",
    "    A^{[1]} &= g^{[1]}(Z^{[1]})\\\\\n",
    "    \\\\\n",
    "    Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]}\\\\\n",
    "    A^{[2]} &= g^{[2]}(Z^{[2]})\\\\\n",
    "\\end{align}\n",
    "\n",
    "`Generalized`\n",
    "\\begin{align}\n",
    "    Z^{[l]} &= W^{[l]} A^{[l-1]} + b^{[l]}\\\\\n",
    "    A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC,abstractmethod\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # number of nodes\n",
    "m = 10 # number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 3), (3, 10), (3, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.random((m, n))\n",
    "w = np.random.random((n, m))\n",
    "b = np.random.random((n_0, 1))\n",
    "a.shape, w.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3),\n",
       " array([[3.75584   , 3.19542765, 3.45235247],\n",
       "        [3.84581895, 3.21067155, 3.1553681 ],\n",
       "        [4.1281608 , 3.67835823, 3.57336282]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = (w @ a) + b\n",
    "z.shape, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\text{input to this layer (for backward propogation)}\\\\\n",
    "    dZ' &= A - y \\text{ (basically difference or inaccuracy or loss on target value)}\\\\\n",
    "    \\\\\n",
    "    & \\text{param for this layer  (this function starts working from here)}\\\\\n",
    "    dW &= dZ .A^T\\\\\n",
    "    dB &= \\sum(dZ)\\\\\n",
    "    \\\\\n",
    "    & \\text{input for next layer (in backward propogation)}\\\\\n",
    "    dZ &= dZ' .W^T \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \"\"\"Layer Module\n",
    "    \n",
    "    It is recommended that input data X is scaled(data scaling operations)\n",
    "    so that data is normalized but meaning of the data remains same.\n",
    "\n",
    "    Attributes:\n",
    "        n_inputs (int) : number of inputs \n",
    "        n_neurons (int) : number of neurons\n",
    "    \"\"\"\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs,n_neurons) # multiply by 0.1 to make it small\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        self.inputs = None \n",
    "        self.output = None\n",
    "        self.dweights = None \n",
    "        self.dbiases = None\n",
    "        self.dinputs = None \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"forward propogation calculation\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray) : X Input matrix\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"backward pass\n",
    "\n",
    "        Args:\n",
    "            dvalues (numpy.ndarray) : gradient value from the next layer to update this layers parameters\n",
    "        \"\"\"\n",
    "        # gradient on parameters \n",
    "        self.dweights = np.dot(self.inputs.T * dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # gradient on values / input to next layer in backpropogation\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    \"\"\"Loss Meta class \n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        \"\"\"mandatory method for child class\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "        \"\"\"Calculate mean loss\n",
    "        \n",
    "        Args:\n",
    "            output : output from the layer\n",
    "            y : truth value/ target/ expected outcome\n",
    "        \"\"\"\n",
    "        # it can be individual outcome of different kind of loss functions\n",
    "        sample_losses = self.forward(output, y) \n",
    "        \n",
    "        # calculating mean\n",
    "        data_loss = np.mean(sample_losses) \n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross Entropy\n",
    "\n",
    "\\begin{align}\n",
    "    L(a^{[l]},y) &= y log(a^{[l]}) + (1 - y) log(1 - a^{[l]})\\\\\n",
    "    \\\\\n",
    "    \\text{derivative of loss over a --> da}\\\\\n",
    "    \\frac{\\partial L}{\\partial a} &= \\big[ \\frac{y}{a} + \\frac{1 -y}{1 - a}(-1) \\big]\\\\\n",
    "    \\frac{\\partial L}{\\partial a} &= \\big[ \\frac{y}{a} - \\frac{1 -y}{1 - a} \\big]\\\\\n",
    "    \\\\\n",
    "    \\text{derivative of loss over z --> dz}\\\\\n",
    "    \\frac{\\partial L}{\\partial z} &= \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial z}\\\\\n",
    "    \\\\\n",
    "    \\text{derivative of loss over w --> dw}\\\\\n",
    "    \\frac{\\partial L}{\\partial w} &= \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w}\n",
    "    \\\\\n",
    "    \\text{derivative of loss over b --> db}\\\\\n",
    "    \\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial b}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* y_pred_clipped\n",
    "    * numpy.clip is used to clip the values from min and max values like bandpass filter\n",
    "    * min = 1.0 * 10^-7 \n",
    "    * max = 1 - 1.0 * 10^-7\n",
    "    \n",
    "\n",
    "* correct_confidences \n",
    "    * probabilities for target value that has been \n",
    "    * calculated earlier \n",
    "    * only for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \"\"\"Categorical Cross entropy loss \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"forward propogation calculation \n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray) : predictions generated\n",
    "            y_true (numpy.ndarray) : actual values\n",
    "        \"\"\"\n",
    "\n",
    "        # get total number of rows/samples\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        \n",
    "        correct_confidences = None\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        \n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis = 1\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # losses\n",
    "        negative_log_Likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Activation Functions \n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Introducing non linearity to the network. Why?\n",
    "2. According to me we need one parameter to compare all the nodes results after learning and passing the value to upcoming nodes.\n",
    "3. To make sense of the data and a mapping for approximation.\n",
    "4. Understand what is the impact of weights and biases changing value to the network/nodes.\n",
    "    If there is only linear fx then it can only fit linear data but if we have not linear data like a sine wave then it will fail to do so. \n",
    "5. If there is no activate function then the whole network will be similar to a one linear node.\n",
    "\n",
    "$w^T(w^T (w^T  x + b) + b) + b ... = output$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise\n",
    "\n",
    "* non granular \n",
    "* only 0 and 1\n",
    "\n",
    "### Backward\n",
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Stepwise:\n",
    "    \"\"\"Stepwise Activation Fx\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply Stepwise to inputs\n",
    "\n",
    "        Args:\n",
    "            inputs (numpy.ndarray) : input matrix\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "$f(x) = \\frac{1}{(1 + e^{-x})}$\n",
    "\n",
    "* granular\n",
    "* between 0 and 1\n",
    "* Comparatively complex calcultaion\n",
    "\n",
    "\n",
    "### Backward\n",
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    \"\"\"Sigmoid Activation Fx\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply Sigmoid to input\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray) : input matrix\n",
    "        \"\"\"\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu\n",
    "\n",
    "\n",
    "$f(x) = 0  | if x \\leq 0$\n",
    "\n",
    "$f(x) = x  | if x \\gt 0$\n",
    "\n",
    "* granular\n",
    "* between 0 to x\n",
    "* easy calculation \n",
    "* almost linear but rectified so less than zeros are not allowed.so introducing slight non linearity makes it eligible for an activation function but also inherently easy and fast calculation than sigmoid.\n",
    "\n",
    "\n",
    "### Backward\n",
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \"\"\"ReLU Activation Fx\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "        self.dinputs = None \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply ReLU to input\n",
    "        \n",
    "        Args:\n",
    "            inputs (numpy.ndarray) : input matrix\n",
    "        \"\"\"\n",
    "        self.inputs = inputs # save inputs \n",
    "        self.output = np.maximum(0, inputs) # calculate from inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Apply backward propogation\n",
    "\n",
    "        Args:\n",
    "            dvalues (numpy.ndarray) : inputs from previous later in backward prop\n",
    "        \"\"\"\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "### Backward\n",
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward propogation calculation\n",
    "\n",
    "        Args:\n",
    "            inputs (numpy.ndarray) : input matrix\n",
    "        \"\"\"\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilites = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilites\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"backward pass\n",
    "\n",
    "        Args:\n",
    "            dvalues (numpy.ndarray) : gradient values\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # everytime random result will be same based on seed\n",
    "\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3, 2.5],\n",
    "    [2.0, 5.0,1.0, 2.0],\n",
    "    [-1.5,2.7,3.3,-0.8],\n",
    "    [1.5,2.0,1.2,3.3]\n",
    "    ])  # (4,4)\n",
    "\n",
    "y = np.array([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration: 0 loss: 1.3601410926262265 acc: 0.1875\n",
      "--------------------------------------------------\n",
      "New set of weights found, iteration: 1 loss: 1.3581512940080587 acc: 0.0625\n",
      "--------------------------------------------------\n",
      "New set of weights found, iteration: 4 loss: 1.3520130233335044 acc: 0.1875\n",
      "--------------------------------------------------\n",
      "New set of weights found, iteration: 5 loss: 1.3262671086535247 acc: 0.0625\n",
      "--------------------------------------------------\n",
      "New set of weights found, iteration: 7 loss: 1.32343642092605 acc: 0.0\n",
      "--------------------------------------------------\n",
      "New set of weights found, iteration: 8 loss: 1.312095358934414 acc: 0.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dense1 = Layer_Dense(4,5) # hidden layer 1 with 4 inputs and 5 neurons \n",
    "activation1 = Activation_ReLU() # layer1 activation function\n",
    "\n",
    "\n",
    "\n",
    "dense2 = Layer_Dense(5,4) # hidden layer 2 with 5 inputs and 4 neurons\n",
    "activation2 = Activation_Softmax() # layer 2 activation function\n",
    "\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy() # loss function for this network training\n",
    "\n",
    "\n",
    "# Helper variables for model\n",
    "lowest_loss = 1e+7\n",
    "best_dense1_weigths = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    ############### here new code will come #######################\n",
    "    dense1.weights += 0.05 * np.random.randn(4, 5)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 5)\n",
    "    dense2.weights += 0.05 * np.random.randn(5, 4)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 4)\n",
    "    ###############################################################\n",
    "\n",
    "\n",
    "    dense1.forward(X)  # forward prop for layer 1\n",
    "    activation1.forward(dense1.output)  # apply activation 1\n",
    "\n",
    "\n",
    "    dense2.forward(activation1.output)  # forward prop for layer 2\n",
    "    activation2.forward(dense2.output)  # apply activation 2\n",
    "\n",
    "    # calculate loss \n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # calculate predictions \n",
    "    # np.argmax returns maximum value over an axis\n",
    "    # so it will return max value from the axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "\n",
    "\n",
    "    # calcuate accuracy\n",
    "    # mean of all the matching predictions to the original \n",
    "    # target value\n",
    "    # basic accuracy calculation\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # if loss is less than move these hyper parameters to best one yet\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "            'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    # Revert weights and biases if loss is not less than previous one \n",
    "    # go back last good one \n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python383jvsc74a57bd01da5964c5502736b4e0a0c4398fb3b913682175f516e99bd48540f11726a612c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
